# awesome-LLM-Quantization
Collect LLM Quantization related papers, data, repositories


## Papers

### 2024

 - [[ICML](https://arxiv.org/abs/2402.04291)] BiLLM: Pushing the Limit of Post-Training Quantization for LLMs [[code](https://github.com/Aaronhuang-778/BiLLM)]![GitHub Repo stars](https://img.shields.io/github/stars/Aaronhuang-778/BiLLM)
 - [[ICML](https://arxiv.org/abs/2402.04396)] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks [[code](https://github.com/Cornell-RelaxML/quip-sharp)]![GitHub Repo stars](https://img.shields.io/github/stars/Cornell-RelaxML/quip-sharp)
 - [[ICML](https://arxiv.org/abs/2403.06082)] FrameQuant: Flexible Low-Bit Quantization for Transformers
 - [[ICML](https://arxiv.org/abs/2306.07629)] SqueezeLLM: Dense-and-Sparse Quantization [[code](https://github.com/SqueezeAILab/SqueezeLLM)]![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM)
 - [[ICML](https://arxiv.org/abs/2401.06118v2)] Extreme Compression of Large Language Models via Additive Quantization [[code](https://github.com/vahe1994/AQLM)]![GitHub Repo stars](https://img.shields.io/github/stars/vahe1994/AQLM)
 - [[ICML](https://arxiv.org/abs/2402.02750)] KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache [[code](https://github.com/jy-yuan/KIVI)]![GitHub Repo stars](https://img.shields.io/github/stars/jy-yuan/KIVI)
 - [[ICML](https://icml.cc/virtual/2024/poster/34619)] BiE: Bi-Exponent Block Floating-Point for Large Language Models Quantization
 - [[ICML](https://icml.cc/virtual/2024/poster/32921)] Compressing Large Language Models by Joint Sparsification and Quantization [[code](https://github.com/uanu2002/JSQ)]![GitHub Repo stars](https://img.shields.io/github/stars/uanu2002/JSQ)
 - [[ICML](https://arxiv.org/abs/2402.02446)] LQER: Low-Rank Quantization Error Reconstruction for LLMs [[code](https://github.com/ChengZhang-98/lqer)]![GitHub Repo stars](https://img.shields.io/github/stars/ChengZhang-98/lqer)
 - [[ICML](https://arxiv.org/abs/2402.05445)] Accurate LoRA-Finetuning Quantization of LLMs via Information Retention [[code](https://github.com/htqin/ir-qlora)]![GitHub Repo stars](https://img.shields.io/github/stars/htqin/ir-qlora)
 - [[ACL](https://arxiv.org/pdf/2402.11960)] DB-LLM: Accurate Dual-Binarization for Efficient LLMs [[code]()]![GitHub Repo stars]()
### 2023
 - [[ICML](https://arxiv.org/abs/2211.10438)] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models [[code](https://github.com/mit-han-lab/smoothquant)]![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/smoothquant)
 - [[EMNLP](https://arxiv.org/abs/2304.09145)] Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling [[code](https://github.com/ModelTC/Outlier_Suppression_Plus)]![GitHub Repo stars](https://img.shields.io/github/stars/ModelTC/Outlier_Suppression_Plus)
 - [[EMNLP](https://arxiv.org/abs/2403.02775)] EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs 
 - [[EMNLP](https://arxiv.org/abs/2310.13315)] Zero-shot Sharpness-Aware Quantization for Pre-trained Language Models 
 - [[EMNLP](https://aclanthology.org/2023.emnlp-main.892/)] A Frustratingly Easy Post-Training Quantization Scheme for LLMs [[code](https://github.com/SamsungLabs/Z-Fold)]![GitHub Repo stars](https://img.shields.io/github/stars/SamsungLabs/Z-Fold)
 - [[EMNLP](https://arxiv.org/abs/2311.05161)] Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization

