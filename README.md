# awesome-LLM-Quantization
Collect LLM Quantization related papers, data, repositories


## Papers

### 2024

 - [[ICML](https://arxiv.org/abs/2402.04291)] BiLLM: Pushing the Limit of Post-Training Quantization for LLMs [[code](https://github.com/Aaronhuang-778/BiLLM)]![GitHub Repo stars](https://img.shields.io/github/stars/Aaronhuang-778/BiLLM)
 - [[ICML](https://arxiv.org/abs/2402.04396)] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks [[code](https://github.com/Cornell-RelaxML/quip-sharp)]![GitHub Repo stars](https://img.shields.io/github/stars/Cornell-RelaxML/quip-sharp)
 - [[ICML](https://arxiv.org/abs/2403.06082)] FrameQuant: Flexible Low-Bit Quantization for Transformers
 - [[ICML](https://arxiv.org/abs/2306.07629)] SqueezeLLM: Dense-and-Sparse Quantization [[code](https://github.com/SqueezeAILab/SqueezeLLM)]![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM)
 - [[ICML](https://arxiv.org/abs/2401.06118v2)] Extreme Compression of Large Language Models via Additive Quantization [[code](https://github.com/vahe1994/AQLM)]![GitHub Repo stars](https://img.shields.io/github/stars/vahe1994/AQLM)
 - [[ICML](https://arxiv.org/abs/2402.02750)] KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache [[code](https://github.com/jy-yuan/KIVI)]![GitHub Repo stars](https://img.shields.io/github/stars/jy-yuan/KIVI)
 - [[ICML](https://icml.cc/virtual/2024/poster/34619)] BiE: Bi-Exponent Block Floating-Point for Large Language Models Quantization
 - [[ICML](https://icml.cc/virtual/2024/poster/32921)] Compressing Large Language Models by Joint Sparsification and Quantization [[code](https://github.com/uanu2002/JSQ)]![GitHub Repo stars](https://img.shields.io/github/stars/uanu2002/JSQ)
 - [[ICML](https://arxiv.org/abs/2402.02446)] LQER: Low-Rank Quantization Error Reconstruction for LLMs [[code](https://github.com/ChengZhang-98/lqer)]![GitHub Repo stars](https://img.shields.io/github/stars/ChengZhang-98/lqer)
 - [[ICML](https://arxiv.org/abs/2402.05445)] Accurate LoRA-Finetuning Quantization of LLMs via Information Retention [[code](https://github.com/htqin/ir-qlora)]![GitHub Repo stars](https://img.shields.io/github/stars/htqin/ir-qlora)
 - [[ACL](https://arxiv.org/pdf/2402.11960)] DB-LLM: Accurate Dual-Binarization for Efficient LLMs
 - [[ACL](https://arxiv.org/pdf/2401.07159)] Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models [[code](https://github.com/youarespecialtome/qst)]![GitHub Repo stars](https://img.shields.io/github/stars/youarespecialtome/qst)
 - [[ACL](https://arxiv.org/pdf/2407.11534)] LRQuant: Learnable and Robust Post-Training Quantization for Large Language Models [[code](https://github.com/zjq0455/RLQ)]![GitHub Repo stars](https://img.shields.io/github/stars/zjq0455/RLQ)
 - [[ACL](https://arxiv.org/pdf/2405.12591)] Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression [[code](https://github.com/lpyhdzx/DecoQuant_code)]![GitHub Repo stars](https://img.shields.io/github/stars/lpyhdzx/DecoQuant_code)
 - [[ACL](https://arxiv.org/pdf/2402.10631)] BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation [[code](https://github.com/DD-DuDa/BitDistiller)]![GitHub Repo stars](https://img.shields.io/github/stars/DD-DuDa/BitDistiller)
 - [[ACL](https://arxiv.org/pdf/2407.03051)] Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment
 - [[NeurIPS](https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2F5c805adc-b555-499f-9882-5ca35ce674b5.pdf)] LLM-MQ: Mixed-precision Quantization for Efficient LLM Deployment
 - [[NeurIPS](https://arxiv.org/pdf/2310.08659)] LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models [[code](https://github.com/yxli2123/LoftQ)]![GitHub Repo stars](https://img.shields.io/github/stars/yxli2123/LoftQ)
 - [[NeurIPS](https://arxiv.org/pdf/2308.09723)] FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs
 - [[ICLR](https://arxiv.org/pdf/2308.13137)] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models [[code](https://github.com/OpenGVLab/OmniQuant)]![GitHub Repo stars](https://img.shields.io/github/stars/OpenGVLab/OmniQuant)
 - [[ICLR](https://arxiv.org/pdf/2310.08659)] LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models [[code](https://github.com/yxli2123/LoftQ)]
 - [[ICLR](https://arxiv.org/pdf/2309.15531)] Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models [[code](https://github.com/johnheo/adadim-llm)]
 - [[ICLR](https://arxiv.org/pdf/2403.12544)] AffineQuant: Affine Transformation Quantization for Large Language Models [[code](https://github.com/bytedance/AffineQuant)]
 - [[ICLR](https://arxiv.org/pdf/2306.03078)] SpQR: ASparse-Quantized Representation for Near-Lossless LLM Weight Compression [[code](https://github.com/Vahe1994/SpQR)]
 - [[ICLR](https://arxiv.org/pdf/2310.08041)] QLLM: ACCURATE AND EFFICIENT LOW-BITWIDTH QUANTIZATION FOR LARGE LANGUAGE MODELS [[code](https://github.com/ziplab/QLLM)]
 - [[ICLR](https://arxiv.org/pdf/2310.01382)] COMPRESSING LLMS: THE TRUTH IS RARELY PURE AND NEVER SIMPLE [[code](https://github.com/VITA-Group/llm-kick)]
 - [[ICLR](https://arxiv.org/pdf/2311.12023)] LQ-LORA: LOW-RANK PLUS QUANTIZED MATRIX DECOMPOSITION FOR EFFICIENT LANGUAGE MODEL FINETUNING [[code](https://github.com/HanGuo97/lq-lora)]
 - [[ICLR](https://arxiv.org/pdf/2309.14717)] QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models [[code](https://github.com/yuhuixu1993/qa-lora)]
 - [[NAACL](https://arxiv.org/pdf/2403.16187)] ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models
 - [[NAACL](https://arxiv.org/pdf/2311.01544)] Divergent Token Metrics: Measuring degradation to prune away LLM components â€“ and optimize quantization
 - [[NAACL](https://arxiv.org/pdf/2406.16299)] Compensate Quantization Errors: Make Weights Hierarchical to Compensate Each Other
### 2023
 - [[ICML](https://arxiv.org/abs/2211.10438)] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models [[code](https://github.com/mit-han-lab/smoothquant)]![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/smoothquant)
 - [[EMNLP](https://arxiv.org/abs/2304.09145)] Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling [[code](https://github.com/ModelTC/Outlier_Suppression_Plus)]![GitHub Repo stars](https://img.shields.io/github/stars/ModelTC/Outlier_Suppression_Plus)
 - [[EMNLP](https://arxiv.org/abs/2403.02775)] EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs 
 - [[EMNLP](https://arxiv.org/abs/2310.13315)] Zero-shot Sharpness-Aware Quantization for Pre-trained Language Models 
 - [[EMNLP](https://aclanthology.org/2023.emnlp-main.892/)] A Frustratingly Easy Post-Training Quantization Scheme for LLMs [[code](https://github.com/SamsungLabs/Z-Fold)]![GitHub Repo stars](https://img.shields.io/github/stars/SamsungLabs/Z-Fold)
 - [[EMNLP](https://arxiv.org/abs/2311.05161)] Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization
 - [[ACL](https://arxiv.org/pdf/2307.05972)] Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models
 - [[NeurIPS](https://arxiv.org/pdf/2305.14152)] Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization
 - [[NeurIPS](https://openreview.net/pdf?id=xrk9g5vcXR)] QuIP: 2-Bit Quantization of Large Language Models With Guarantees [[code](https://github.com/jerry-chee/QuIP)]![GitHub Repo stars](https://img.shields.io/github/stars/jerry-chee/QuIP)
 - [[NeurIPS](https://arxiv.org/pdf/2306.12929)] Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing [[code](https://github.com/Qualcomm-AI-research/outlier-free-transformers)]![GitHub Repo stars](https://img.shields.io/github/stars/Qualcomm-AI-research/outlier-free-transformers)
 - [[NeurIPS](https://arxiv.org/pdf/2305.14314)] QLoRA: Efficient Finetuning of Quantized LLMs [[code](https://github.com/artidoro/qlora)]![GitHub Repo stars](https://img.shields.io/github/stars/artidoro/qlora)
 - [[NeurIPS](https://arxiv.org/pdf/2306.11987)] Training Transformers with 4-bit Integers
 - [[NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2023/file/0113ef4642264adc2e6924a3cbbdf532-Paper-Conference.pdf)] TexQ: Zero-shot Network Quantization with Texture Feature Distribution Calibration
 - [[ACL](https://arxiv.org/pdf/2306.00014)] PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models
 - [[ICLR](https://arxiv.org/pdf/2210.17323)] OPTQ: Accurate Quantization for Generative Pre-trained Transformers  [[code](https://github.com/ist-daslab/gptq)]
 - [[ICLR](https://arxiv.org/pdf/2210.08502)] FIT: A Metric for Model Sensitivity 
 - [[ICLR](https://arxiv.org/pdf/2301.09858)] PowerQuant: Automorphism Search for Non-Uniform Quantization
 - [[ICLR](https://arxiv.org/pdf/2112.10769)] Accurate Neural Training with 4-bit Matrix Multiplications at Standard Formats 
### 2022
- [[NeurIPS](https://arxiv.org/pdf/2206.01861)] ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers [[code](https://github.com/microsoft/DeepSpeed)]![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/DeepSpeed)
- [[NeurIPS](https://arxiv.org/pdf/2208.07339)] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scal [[code](https://github.com/bitsandbytes-foundation/bitsandbytes)]![GitHub Repo stars](https://img.shields.io/github/stars/bitsandbytes-foundation/bitsandbytes)
- [[NeurIPS](https://arxiv.org/pdf/2209.13325)] Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models [[code](https://github.com/wimh966/outlier_suppression)]![GitHub Repo stars](https://img.shields.io/github/stars/wimh966/outlier_suppression)
